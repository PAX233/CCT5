{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 将数据转为  BIO 标注形式\n",
    "def dimension_label(path, save_path, labels_path=None):\n",
    "    label_dict = ['O']\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as w:\n",
    "        #写入模式\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as r:\n",
    "            for line in r:\n",
    "                # print(line)\n",
    "                line = json.loads(line)\n",
    "                text = line['text']\n",
    "                label = line['label']\n",
    "                text_label = ['O'] * len(text)\n",
    "                for label_key in label:  # 遍历实体标签\n",
    "                    B_label = \"B-\" + label_key\n",
    "                    I_label = \"I-\" + label_key\n",
    "                    if B_label not in label_dict:\n",
    "                        label_dict.append(B_label)\n",
    "                    if I_label not in label_dict:\n",
    "                        label_dict.append(I_label)\n",
    "                    label_item = label[label_key]\n",
    "                    for entity in label_item:  # 遍历实体\n",
    "                        position = label_item[entity]\n",
    "                        '''\n",
    "                        start = position[0][0]\n",
    "                        end = position[0][1]\n",
    "                        print(f\"实体: {entity}, 起始位置: {start}, 结束位置: {end}, 文本: {text}, 文本标签长度: {len(text_label)}\")\n",
    "                        # 检查 start 和 end 是否在 text_label 的范围内\n",
    "                        if start < 0 or end >= len(text_label):\n",
    "                            print(f\"错误：实体 {entity} 的索引超出范围。起始位置: {start}, 结束位置: {end}, 文本长度: {len(text_label)}\")\n",
    "                            continue\n",
    "                        text_label[start] = B_label\n",
    "                        for i in range(start + 1, end + 1):\n",
    "                            text_label[i] = I_label\n",
    "                        '''\n",
    "                        start = position[0]\n",
    "                        end = position[1]\n",
    "                        text_label[start] = B_label\n",
    "                        # print(start)\n",
    "                        # print(end)\n",
    "                        # print(len(text_label))\n",
    "                        # print('-'*100)\n",
    "                        for i in range(start + 1, end + 1):\n",
    "                            text_label[i] = I_label\n",
    "                line = {\n",
    "                    \"text\": text,\n",
    "                    \"label\": text_label\n",
    "                }\n",
    "                line = json.dumps(line, ensure_ascii=False)\n",
    "                w.write(line + \"\\n\")\n",
    "                w.flush()\n",
    "\n",
    "    if labels_path:  # 保存 label ，后续训练和预测时使用\n",
    "        label_map = {}\n",
    "        for i,label in enumerate(label_dict):\n",
    "            label_map[label] = i\n",
    "        with open(labels_path, \"w\", encoding=\"utf-8\") as w:\n",
    "            labels = json.dumps(label_map, ensure_ascii=False)\n",
    "            w.write(labels + \"\\n\")\n",
    "            w.flush()\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    path = \"./data/NER/dev.json\"\n",
    "    save_path = \"./data/NER/new/dev.json\"\n",
    "    dimension_label(path, save_path)\n",
    "\n",
    "    path = \"./data/NER/train.json\"\n",
    "    save_path = \"./data/NER/new/train.json\"\n",
    "    dimension_label(path, save_path)'''\n",
    "    \n",
    "\n",
    "    data_path = \"./data/NER/NERforRE.json\"\n",
    "    save_path = \"./data/NER/new/NER_BIO.json\"\n",
    "    labels_path = \"./data/NER/new/labels.json\"\n",
    "    \n",
    "    \n",
    "    with open(data_path,'r') as w:\n",
    "        content = w.read()\n",
    "    with open(data_path,'w') as w:\n",
    "        content = re.sub(\"'\",'\"',content)\n",
    "        w.write(content)\n",
    "    \n",
    "    dimension_label(data_path,save_path,labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, labels_map, max_length=300):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels_map = labels_map\n",
    "\n",
    "        self.text_data = []\n",
    "        self.label_data = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as r:\n",
    "            for line in r:\n",
    "                line = json.loads(line)\n",
    "                text = line['text']\n",
    "                label = line['label']\n",
    "                self.text_data.append(text)\n",
    "                self.label_data.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        labels = self.label_data[idx]\n",
    "\n",
    "        # 使用分词器对句子进行处理\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        # 将标签转换为数字编码\n",
    "        label_ids = [self.labels_map[l] for l in labels]\n",
    "\n",
    "        if len(label_ids) > self.max_length:\n",
    "            label_ids = label_ids[0:self.max_length]\n",
    "\n",
    "        if len(label_ids) < self.max_length:\n",
    "            # 标签填充到最大长度\n",
    "            label_ids.extend([0] * (self.max_length - len(label_ids)))\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.LongTensor(label_ids)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 00:21:08.060034: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-10 00:21:08.878254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./model/roberta/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "0, epoch: 0 -loss: tensor(7.7149, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 13796.501159667969\n",
      "100, epoch: 0 -loss: tensor(1.4496, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.06796803830179812\n",
      "200, epoch: 0 -loss: tensor(0.8947, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.060187452740135934\n",
      "242, epoch: 0 -loss: tensor(0.5422, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05911853116789221\n",
      "Train Epoch: 0: 100%|██████████| 243/243 [00:14<00:00, 16.97it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.95it/s]\n",
      "Validation : acc: 0.0 , f1: 0.15697252316965712\n",
      "0, epoch: 1 -loss: tensor(0.7076, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 514.9459838867188\n",
      "100, epoch: 1 -loss: tensor(0.7236, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05288660959660573\n",
      "200, epoch: 1 -loss: tensor(1.2118, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05264342179268425\n",
      "242, epoch: 1 -loss: tensor(2.0998, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052978788307960004\n",
      "Train Epoch: 1: 100%|██████████| 243/243 [00:12<00:00, 18.93it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.59it/s]\n",
      "Validation : acc: 0.0 , f1: 0.1656856180148699\n",
      "0, epoch: 2 -loss: tensor(2.6564, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 524.146556854248\n",
      "100, epoch: 2 -loss: tensor(0.3539, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05261929972139852\n",
      "200, epoch: 2 -loss: tensor(2.2382, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052437165983031386\n",
      "242, epoch: 2 -loss: tensor(0.2791, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05268675719565828\n",
      "Train Epoch: 2: 100%|██████████| 243/243 [00:12<00:00, 19.04it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.48it/s]\n",
      "Validation : acc: 0.0 , f1: 0.198528943808415\n",
      "0, epoch: 3 -loss: tensor(0.3816, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 517.9286003112793\n",
      "100, epoch: 3 -loss: tensor(0.5277, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0529756874570518\n",
      "200, epoch: 3 -loss: tensor(0.3586, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05291059590302419\n",
      "242, epoch: 3 -loss: tensor(1.1472, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053294506591867595\n",
      "Train Epoch: 3: 100%|██████████| 243/243 [00:12<00:00, 18.82it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.83it/s]\n",
      "Validation : acc: 0.015068813216961364 , f1: 0.254607684544333\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 4 -loss: tensor(0.6516, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 528.6264419555664\n",
      "100, epoch: 4 -loss: tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05363386035997509\n",
      "200, epoch: 4 -loss: tensor(0.4265, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053043118419770746\n",
      "242, epoch: 4 -loss: tensor(0.5347, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053197424951055595\n",
      "Train Epoch: 4: 100%|██████████| 243/243 [00:12<00:00, 18.86it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.84it/s]\n",
      "Validation : acc: 0.023324891843410363 , f1: 0.3069462832858969\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 5 -loss: tensor(0.8691, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 527.3890495300293\n",
      "100, epoch: 5 -loss: tensor(0.6685, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052540690810291855\n",
      "200, epoch: 5 -loss: tensor(0.6778, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05233871584838805\n",
      "242, epoch: 5 -loss: tensor(0.9933, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0526206404013119\n",
      "Train Epoch: 5: 100%|██████████| 243/243 [00:12<00:00, 19.06it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.35it/s]\n",
      "Validation : acc: 0.033818719003904193 , f1: 0.339214253255284\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 6 -loss: tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 523.4098434448242\n",
      "100, epoch: 6 -loss: tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052916869652405246\n",
      "200, epoch: 6 -loss: tensor(1.4643, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052504063818692576\n",
      "242, epoch: 6 -loss: tensor(0.3980, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05274656476707226\n",
      "Train Epoch: 6: 100%|██████████| 243/243 [00:12<00:00, 19.02it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.66it/s]\n",
      "Validation : acc: 0.0474544888125135 , f1: 0.3907206242257942\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 7 -loss: tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 526.9122123718262\n",
      "100, epoch: 7 -loss: tensor(0.3868, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05419333522240574\n",
      "200, epoch: 7 -loss: tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05339851808910155\n",
      "242, epoch: 7 -loss: tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0536007531597739\n",
      "Train Epoch: 7: 100%|██████████| 243/243 [00:12<00:00, 18.71it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.93it/s]\n",
      "Validation : acc: 0.1216622198103679 , f1: 0.44621583808999654\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 8 -loss: tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 748.1169700622559\n",
      "100, epoch: 8 -loss: tensor(0.2535, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05393715948370843\n",
      "200, epoch: 8 -loss: tensor(0.5517, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05325636108239075\n",
      "242, epoch: 8 -loss: tensor(0.5287, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05342774902570135\n",
      "Train Epoch: 8: 100%|██████████| 243/243 [00:12<00:00, 18.78it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.15it/s]\n",
      "Validation : acc: 0.1508248039112236 , f1: 0.4851075739084984\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 9 -loss: tensor(0.4767, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 523.6649513244629\n",
      "100, epoch: 9 -loss: tensor(0.3374, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053256944805188985\n",
      "200, epoch: 9 -loss: tensor(0.2633, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0526687644828785\n",
      "242, epoch: 9 -loss: tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052924494902809725\n",
      "Train Epoch: 9: 100%|██████████| 243/243 [00:12<00:00, 18.95it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.07it/s]\n",
      "Validation : acc: 0.21602262620781146 , f1: 0.5351253711943624\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 10 -loss: tensor(0.3330, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 524.3444442749023\n",
      "100, epoch: 10 -loss: tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05318529294126345\n",
      "200, epoch: 10 -loss: tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05269483554146773\n",
      "242, epoch: 10 -loss: tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05287141132366834\n",
      "Train Epoch: 10: 100%|██████████| 243/243 [00:12<00:00, 18.97it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.94it/s]\n",
      "Validation : acc: 0.25295628011677407 , f1: 0.577211184988436\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 11 -loss: tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 545.3300476074219\n",
      "100, epoch: 11 -loss: tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05308216985636774\n",
      "200, epoch: 11 -loss: tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052630418449881713\n",
      "242, epoch: 11 -loss: tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052856600807062636\n",
      "Train Epoch: 11: 100%|██████████| 243/243 [00:12<00:00, 18.98it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.94it/s]\n",
      "Validation : acc: 0.2826512307993791 , f1: 0.6051562992548587\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 12 -loss: tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 526.5522003173828\n",
      "100, epoch: 12 -loss: tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053300780401307\n",
      "200, epoch: 12 -loss: tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052682080153863484\n",
      "242, epoch: 12 -loss: tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052897040325130526\n",
      "Train Epoch: 12: 100%|██████████| 243/243 [00:12<00:00, 18.96it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.93it/s]\n",
      "Validation : acc: 0.3051037097333392 , f1: 0.6366623655220413\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 13 -loss: tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 525.362491607666\n",
      "100, epoch: 13 -loss: tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053699234668990424\n",
      "200, epoch: 13 -loss: tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05293757175966871\n",
      "242, epoch: 13 -loss: tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05320625628684044\n",
      "Train Epoch: 13: 100%|██████████| 243/243 [00:12<00:00, 18.85it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.04it/s]\n",
      "Validation : acc: 0.35406510499103083 , f1: 0.6750404723857699\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 14 -loss: tensor(0.2738, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 792.6321029663086\n",
      "100, epoch: 14 -loss: tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05438236997218325\n",
      "200, epoch: 14 -loss: tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053515161936419274\n",
      "242, epoch: 14 -loss: tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053624565451822065\n",
      "Train Epoch: 14: 100%|██████████| 243/243 [00:12<00:00, 18.71it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.11it/s]\n",
      "Validation : acc: 0.3976636417994443 , f1: 0.6951864223615816\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 15 -loss: tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 526.740550994873\n",
      "100, epoch: 15 -loss: tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05350400342653856\n",
      "200, epoch: 15 -loss: tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052888318082131854\n",
      "242, epoch: 15 -loss: tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05304496519952671\n",
      "Train Epoch: 15: 100%|██████████| 243/243 [00:12<00:00, 18.91it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.01it/s]\n",
      "Validation : acc: 0.46039798107699337 , f1: 0.7228013171721112\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 16 -loss: tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 525.6772041320801\n",
      "100, epoch: 16 -loss: tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052992677148008885\n",
      "200, epoch: 16 -loss: tensor(0.3622, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05234551673495647\n",
      "242, epoch: 16 -loss: tensor(0.0087, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05260898252577172\n",
      "Train Epoch: 16: 100%|██████████| 243/243 [00:12<00:00, 19.07it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.99it/s]\n",
      "Validation : acc: 0.44183373782139207 , f1: 0.7362928793186531\n",
      "0, epoch: 17 -loss: tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 522.6802825927734\n",
      "100, epoch: 17 -loss: tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05245762347621441\n",
      "200, epoch: 17 -loss: tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052193917165141636\n",
      "242, epoch: 17 -loss: tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05237778654883128\n",
      "Train Epoch: 17: 100%|██████████| 243/243 [00:12<00:00, 19.15it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.55it/s]\n",
      "Validation : acc: 0.4606151735781363 , f1: 0.7497847668740143\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 18 -loss: tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 512.1016502380371\n",
      "100, epoch: 18 -loss: tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05258846984576477\n",
      "200, epoch: 18 -loss: tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05223503958110386\n",
      "242, epoch: 18 -loss: tensor(0.3176, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05249378994723852\n",
      "Train Epoch: 18: 100%|██████████| 243/243 [00:12<00:00, 19.11it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.19it/s]\n",
      "Validation : acc: 0.48680238371596396 , f1: 0.7610676748995201\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 19 -loss: tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 525.7153511047363\n",
      "100, epoch: 19 -loss: tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052575635786485796\n",
      "200, epoch: 19 -loss: tensor(0.0252, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05217567219249838\n",
      "242, epoch: 19 -loss: tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052440099484692033\n",
      "Train Epoch: 19: 100%|██████████| 243/243 [00:12<00:00, 19.13it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.01it/s]\n",
      "Validation : acc: 0.4800465538119859 , f1: 0.7743912551740032\n",
      "0, epoch: 20 -loss: tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 516.2620544433594\n",
      "100, epoch: 20 -loss: tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05276998726526053\n",
      "200, epoch: 20 -loss: tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052652737994004536\n",
      "242, epoch: 20 -loss: tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05296274629093806\n",
      "Train Epoch: 20: 100%|██████████| 243/243 [00:12<00:00, 18.94it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.94it/s]\n",
      "Validation : acc: 0.5020695468226333 , f1: 0.7908266321168872\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 21 -loss: tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 546.3409423828125\n",
      "100, epoch: 21 -loss: tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05295902916758826\n",
      "200, epoch: 21 -loss: tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05252288814819338\n",
      "242, epoch: 21 -loss: tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052837564766228765\n",
      "Train Epoch: 21: 100%|██████████| 243/243 [00:12<00:00, 18.98it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.19it/s]\n",
      "Validation : acc: 0.5668255726897701 , f1: 0.8089134233735276\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 22 -loss: tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 509.2287063598633\n",
      "100, epoch: 22 -loss: tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05333949000310033\n",
      "200, epoch: 22 -loss: tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052733359196217126\n",
      "242, epoch: 22 -loss: tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05292917164879039\n",
      "Train Epoch: 22: 100%|██████████| 243/243 [00:12<00:00, 18.95it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.67it/s]\n",
      "Validation : acc: 0.5650080143907306 , f1: 0.8201040681005716\n",
      "0, epoch: 23 -loss: tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 515.3083801269531\n",
      "100, epoch: 23 -loss: tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05311088257713622\n",
      "200, epoch: 23 -loss: tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05267444122640853\n",
      "242, epoch: 23 -loss: tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052915851740288476\n",
      "Train Epoch: 23: 100%|██████████| 243/243 [00:12<00:00, 18.96it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.56it/s]\n",
      "Validation : acc: 0.6365492646356845 , f1: 0.8385712100569612\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 24 -loss: tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 525.0835418701172\n",
      "100, epoch: 24 -loss: tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053585654555672345\n",
      "200, epoch: 24 -loss: tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05310244170096116\n",
      "242, epoch: 24 -loss: tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053249883913185865\n",
      "Train Epoch: 24: 100%|██████████| 243/243 [00:12<00:00, 18.84it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.82it/s]\n",
      "Validation : acc: 0.6377201445102677 , f1: 0.844956525119727\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 25 -loss: tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 528.2402038574219\n",
      "100, epoch: 25 -loss: tensor(0.0093, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05291547490511225\n",
      "200, epoch: 25 -loss: tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052802065435896606\n",
      "242, epoch: 25 -loss: tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05300604683077294\n",
      "Train Epoch: 25: 100%|██████████| 243/243 [00:12<00:00, 18.92it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.34it/s]\n",
      "Validation : acc: 0.6667390222945777 , f1: 0.8563275330092072\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 26 -loss: tensor(0.0209, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 577.4831771850586\n",
      "100, epoch: 26 -loss: tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0541835481495207\n",
      "200, epoch: 26 -loss: tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05353971903778819\n",
      "242, epoch: 26 -loss: tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05373226755222361\n",
      "Train Epoch: 26: 100%|██████████| 243/243 [00:13<00:00, 18.67it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 64.61it/s]\n",
      "Validation : acc: 0.6816910113206405 , f1: 0.8593133631924866\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 27 -loss: tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 519.4258689880371\n",
      "100, epoch: 27 -loss: tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05281257593345678\n",
      "200, epoch: 27 -loss: tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052562529985519676\n",
      "242, epoch: 27 -loss: tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05282600836632289\n",
      "Train Epoch: 27: 100%|██████████| 243/243 [00:12<00:00, 18.99it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.57it/s]\n",
      "Validation : acc: 0.6857425496314383 , f1: 0.8655602758894414\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 28 -loss: tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 550.5800247192383\n",
      "100, epoch: 28 -loss: tensor(0.0206, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05340237999300575\n",
      "200, epoch: 28 -loss: tensor(0.0113, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05307505934084252\n",
      "242, epoch: 28 -loss: tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0533013832901918\n",
      "Train Epoch: 28: 100%|██████████| 243/243 [00:12<00:00, 18.82it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.76it/s]\n",
      "Validation : acc: 0.7115049216901069 , f1: 0.8740241379603068\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 29 -loss: tensor(0.0113, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 511.3387107849121\n",
      "100, epoch: 29 -loss: tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05308628495692793\n",
      "200, epoch: 29 -loss: tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05296076987581623\n",
      "242, epoch: 29 -loss: tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05334633995178642\n",
      "Train Epoch: 29: 100%|██████████| 243/243 [00:12<00:00, 18.80it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 64.95it/s]\n",
      "Validation : acc: 0.6910659642141124 , f1: 0.8721916671950108\n",
      "0, epoch: 30 -loss: tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 520.3914642333984\n",
      "100, epoch: 30 -loss: tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05320450945952252\n",
      "200, epoch: 30 -loss: tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05284852127280662\n",
      "242, epoch: 30 -loss: tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053118109427848254\n",
      "Train Epoch: 30: 100%|██████████| 243/243 [00:12<00:00, 18.89it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.13it/s]\n",
      "Validation : acc: 0.7570845706648173 , f1: 0.884820487773615\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 31 -loss: tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 522.160530090332\n",
      "100, epoch: 31 -loss: tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05329564725443209\n",
      "200, epoch: 31 -loss: tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05302230448822214\n",
      "242, epoch: 31 -loss: tensor(0.0230, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05319668506560553\n",
      "Train Epoch: 31: 100%|██████████| 243/243 [00:12<00:00, 18.85it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.56it/s]\n",
      "Validation : acc: 0.7239992060979716 , f1: 0.8818066309875349\n",
      "0, epoch: 32 -loss: tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 520.0815200805664\n",
      "100, epoch: 32 -loss: tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05314902951164554\n",
      "200, epoch: 32 -loss: tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05269493090885168\n",
      "242, epoch: 32 -loss: tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05307942553677373\n",
      "Train Epoch: 32: 100%|██████████| 243/243 [00:12<00:00, 18.90it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.67it/s]\n",
      "Validation : acc: 0.7663860373736916 , f1: 0.8908141851671166\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 33 -loss: tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 528.8267135620117\n",
      "100, epoch: 33 -loss: tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05352319610296356\n",
      "200, epoch: 33 -loss: tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053186516394565286\n",
      "242, epoch: 33 -loss: tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05340211312783683\n",
      "Train Epoch: 33: 100%|██████████| 243/243 [00:12<00:00, 18.78it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.47it/s]\n",
      "Validation : acc: 0.780343513368205 , f1: 0.8944080653253274\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 34 -loss: tensor(0.0192, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 712.0156288146973\n",
      "100, epoch: 34 -loss: tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05418871944333011\n",
      "200, epoch: 34 -loss: tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053409294603488686\n",
      "242, epoch: 34 -loss: tensor(0.0134, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0535927267326611\n",
      "Train Epoch: 34: 100%|██████████| 243/243 [00:12<00:00, 18.72it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.38it/s]\n",
      "Validation : acc: 0.7790746519141579 , f1: 0.8985416763338592\n",
      "0, epoch: 35 -loss: tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 520.2078819274902\n",
      "100, epoch: 35 -loss: tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053227857767625626\n",
      "200, epoch: 35 -loss: tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05295878861841453\n",
      "242, epoch: 35 -loss: tensor(0.0234, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05318661434780179\n",
      "Train Epoch: 35: 100%|██████████| 243/243 [00:12<00:00, 18.86it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.31it/s]\n",
      "Validation : acc: 0.7931350085671076 , f1: 0.9039956095536988\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 36 -loss: tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 529.8972129821777\n",
      "100, epoch: 36 -loss: tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052979340026031066\n",
      "200, epoch: 36 -loss: tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05237627986883638\n",
      "242, epoch: 36 -loss: tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0525536152522585\n",
      "Train Epoch: 36: 100%|██████████| 243/243 [00:12<00:00, 19.09it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.88it/s]\n",
      "Validation : acc: 0.7907230286859913 , f1: 0.9040804591541579\n",
      "0, epoch: 37 -loss: tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 511.49845123291016\n",
      "100, epoch: 37 -loss: tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05234747658700217\n",
      "200, epoch: 37 -loss: tensor(0.0062, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05234519963840482\n",
      "242, epoch: 37 -loss: tensor(0.0091, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05266505520272337\n",
      "Train Epoch: 37: 100%|██████████| 243/243 [00:12<00:00, 19.05it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 65.82it/s]\n",
      "Validation : acc: 0.8154829738163072 , f1: 0.9095658460474962\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 38 -loss: tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 520.2674865722656\n",
      "100, epoch: 38 -loss: tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05272474500093249\n",
      "200, epoch: 38 -loss: tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05232825881473717\n",
      "242, epoch: 38 -loss: tensor(0.0235, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05256553814668824\n",
      "Train Epoch: 38: 100%|██████████| 243/243 [00:12<00:00, 19.08it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.92it/s]\n",
      "Validation : acc: 0.8014111859790871 , f1: 0.9102940317346427\n",
      "0, epoch: 39 -loss: tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 517.8427696228027\n",
      "100, epoch: 39 -loss: tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05257109868546259\n",
      "200, epoch: 39 -loss: tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052254304984755466\n",
      "242, epoch: 39 -loss: tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0524913279316064\n",
      "Train Epoch: 39: 100%|██████████| 243/243 [00:12<00:00, 19.11it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.11it/s]\n",
      "Validation : acc: 0.7097575835230154 , f1: 0.8913976930228955\n",
      "0, epoch: 40 -loss: tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 514.7862434387207\n",
      "100, epoch: 40 -loss: tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05252747289748438\n",
      "200, epoch: 40 -loss: tensor(0.0089, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05247982381387532\n",
      "242, epoch: 40 -loss: tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05273041634841436\n",
      "Train Epoch: 40: 100%|██████████| 243/243 [00:12<00:00, 19.02it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.61it/s]\n",
      "Validation : acc: 0.8747079395227546 , f1: 0.925235197222962\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 41 -loss: tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 525.0835418701172\n",
      "100, epoch: 41 -loss: tensor(0.0110, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05328349268742406\n",
      "200, epoch: 41 -loss: tensor(0.0154, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05274400100417442\n",
      "242, epoch: 41 -loss: tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05297899322962926\n",
      "Train Epoch: 41: 100%|██████████| 243/243 [00:12<00:00, 18.93it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.69it/s]\n",
      "Validation : acc: 0.8121450680092658 , f1: 0.9131306796657404\n",
      "0, epoch: 42 -loss: tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 519.4854736328125\n",
      "100, epoch: 42 -loss: tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05312471322507925\n",
      "200, epoch: 42 -loss: tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052696494933948575\n",
      "242, epoch: 42 -loss: tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052922149140816895\n",
      "Train Epoch: 42: 100%|██████████| 243/243 [00:12<00:00, 18.96it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.73it/s]\n",
      "Validation : acc: 0.807206796404327 , f1: 0.9121054302218821\n",
      "0, epoch: 43 -loss: tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 518.7058448791504\n",
      "100, epoch: 43 -loss: tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05271288130430009\n",
      "200, epoch: 43 -loss: tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052617577231631914\n",
      "242, epoch: 43 -loss: tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052861534690330204\n",
      "Train Epoch: 43: 100%|██████████| 243/243 [00:12<00:00, 18.98it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.00it/s]\n",
      "Validation : acc: 0.8171519267198278 , f1: 0.9141479674419525\n",
      "0, epoch: 44 -loss: tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 511.6105079650879\n",
      "100, epoch: 44 -loss: tensor(0.0119, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052800576338370195\n",
      "200, epoch: 44 -loss: tensor(0.5867, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05252056118402483\n",
      "242, epoch: 44 -loss: tensor(0.0128, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052790336232666296\n",
      "Train Epoch: 44: 100%|██████████| 243/243 [00:12<00:00, 19.00it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.20it/s]\n",
      "Validation : acc: 0.8190037785716799 , f1: 0.9152794711095779\n",
      "0, epoch: 45 -loss: tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 571.6609954833984\n",
      "100, epoch: 45 -loss: tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053709832364233795\n",
      "200, epoch: 45 -loss: tensor(0.0143, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05300985069996967\n",
      "242, epoch: 45 -loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05318119673117072\n",
      "Train Epoch: 45: 100%|██████████| 243/243 [00:12<00:00, 18.87it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 67.26it/s]\n",
      "Validation : acc: 0.8263162192174538 , f1: 0.9165672168659158\n",
      "0, epoch: 46 -loss: tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 498.31390380859375\n",
      "100, epoch: 46 -loss: tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052802610046816224\n",
      "200, epoch: 46 -loss: tensor(0.0135, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052540075734967016\n",
      "242, epoch: 46 -loss: tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05280013306477747\n",
      "Train Epoch: 46: 100%|██████████| 243/243 [00:12<00:00, 19.00it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.75it/s]\n",
      "Validation : acc: 0.8480847113563162 , f1: 0.9223629843607085\n",
      "0, epoch: 47 -loss: tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 524.1274833679199\n",
      "100, epoch: 47 -loss: tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05357095606496923\n",
      "200, epoch: 47 -loss: tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053231678811330396\n",
      "242, epoch: 47 -loss: tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053365943468012086\n",
      "Train Epoch: 47: 100%|██████████| 243/243 [00:12<00:00, 18.80it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.16it/s]\n",
      "Validation : acc: 0.8034995754131554 , f1: 0.9129094799294631\n",
      "0, epoch: 48 -loss: tensor(0.0154, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 517.4589157104492\n",
      "100, epoch: 48 -loss: tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05262269241438644\n",
      "200, epoch: 48 -loss: tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05225829014931257\n",
      "242, epoch: 48 -loss: tensor(0.0120, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05257194391925328\n",
      "Train Epoch: 48: 100%|██████████| 243/243 [00:12<00:00, 19.08it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.56it/s]\n",
      "Validation : acc: 0.8872901561173164 , f1: 0.9288364732238648\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 49 -loss: tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 527.5487899780273\n",
      "100, epoch: 49 -loss: tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05340999507480717\n",
      "200, epoch: 49 -loss: tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0529059300537641\n",
      "242, epoch: 49 -loss: tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05312750725382456\n",
      "Train Epoch: 49: 100%|██████████| 243/243 [00:12<00:00, 18.88it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.15it/s]\n",
      "Validation : acc: 0.8793340518649159 , f1: 0.9307835780214219\n",
      "0, epoch: 50 -loss: tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 518.1169509887695\n",
      "100, epoch: 50 -loss: tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05285450418284933\n",
      "200, epoch: 50 -loss: tensor(0.0251, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05258319848180774\n",
      "242, epoch: 50 -loss: tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05278438956345642\n",
      "Train Epoch: 50: 100%|██████████| 243/243 [00:12<00:00, 19.00it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.14it/s]\n",
      "Validation : acc: 0.8617757528251356 , f1: 0.9303727340019089\n",
      "0, epoch: 51 -loss: tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 522.3250389099121\n",
      "100, epoch: 51 -loss: tensor(0.0154, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052316508428730826\n",
      "200, epoch: 51 -loss: tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05249896643202008\n",
      "242, epoch: 51 -loss: tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.0528507329538794\n",
      "Train Epoch: 51: 100%|██████████| 243/243 [00:12<00:00, 18.98it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.42it/s]\n",
      "Validation : acc: 0.9021084882195994 , f1: 0.935940942024665\n",
      "Save Model To  ./output/NER\n",
      "0, epoch: 52 -loss: tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 523.7126350402832\n",
      "100, epoch: 52 -loss: tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05227192658320647\n",
      "200, epoch: 52 -loss: tensor(0.0115, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052313585350623096\n",
      "242, epoch: 52 -loss: tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.052623188129372986\n",
      "Train Epoch: 52: 100%|██████████| 243/243 [00:12<00:00, 19.06it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.57it/s]\n",
      "Validation : acc: 0.8753674309229866 , f1: 0.9288618318646888\n",
      "0, epoch: 53 -loss: tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 520.327091217041\n",
      "100, epoch: 53 -loss: tensor(0.0173, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.053120683955121706\n",
      "200, epoch: 53 -loss: tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05289363242960285\n",
      "242, epoch: 53 -loss: tensor(0.0104, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 0.05312351620727565\n",
      "Train Epoch: 53: 100%|██████████| 243/243 [00:12<00:00, 18.88it/s]\n",
      "Start Validation...\n",
      "Validation Data: 100%|██████████| 243/243 [00:03<00:00, 66.33it/s]\n",
      "Validation : acc: 0.8168218041674832 , f1: 0.9158045028457857\n",
      "0, epoch: 54 -loss: tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>) ; each step's time spent: 519.9837684631348\n",
      "Train Epoch: 54:  21%|██        | 51/243 [00:02<00:10, 18.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1550969/3337182748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mwriter0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1550969/3337182748.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start Validation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1550969/3337182748.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, device, loader, optimizer, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# 反向传播，计算当前梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# 梯度累积步数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "#from ner_datasets import NERDataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "writer0 = SummaryWriter(log_dir = './output/NER/output')\n",
    "\n",
    "def extract_non_zero_sublists(lst):\n",
    "    non_zero_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] != 0:\n",
    "            sublist.append(i)\n",
    "        else:\n",
    "            if sublist: \n",
    "                non_zero_sublists.append(sublist)\n",
    "            sublist = []  \n",
    "    if sublist:\n",
    "        non_zero_sublists.append(sublist)\n",
    "\n",
    "    return non_zero_sublists\n",
    "\n",
    "\n",
    "def train(epoch, model, device, loader, optimizer, gradient_accumulation_steps):\n",
    "    model.train()\n",
    "    time1 = time.time()\n",
    "    loss = 0\n",
    "    for index, data in enumerate(tqdm(loader, file=sys.stdout, desc=\"Train Epoch: \" + str(epoch))):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        # 反向传播，计算当前梯度\n",
    "        loss.backward()\n",
    "        loss += loss.item()\n",
    "        # 梯度累积步数\n",
    "        if (index % gradient_accumulation_steps == 0 and index != 0) or index == len(loader) - 1:\n",
    "            # 更新网络参数\n",
    "            optimizer.step()\n",
    "            # 清空过往梯度\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 100轮打印一次 loss\n",
    "        \n",
    "        if index % 100 == 0 or index == len(loader) - 1:\n",
    "            time2 = time.time()\n",
    "            tqdm.write(\n",
    "                f\"{index}, epoch: {epoch} -loss: {str(loss)} ; each step's time spent: {(str(float(time2 - time1) / float(index + 0.0001)))}\")\n",
    "    writer0.add_scalar('train_loss', loss.item()/len(loader), epoch)\n",
    "\n",
    "\n",
    "def validate(model, device, loader,epoch):\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    f1 = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(tqdm(loader, file=sys.stdout, desc=\"Validation Data\")):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs.logits, dim=2)\n",
    "\n",
    "            predicted_labels = predicted_labels.detach().cpu().numpy().tolist()\n",
    "            true_labels = labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "            predicted_labels_flat = [label for sublist in predicted_labels for label in sublist]\n",
    "            true_labels_flat = [label for sublist in true_labels for label in sublist]\n",
    "            \n",
    "\n",
    "            # accuracy = (np.array(predicted_labels_flat) == np.array(true_labels_flat)).mean()\n",
    "            accuracy = 0\n",
    "            true_index = extract_non_zero_sublists(true_labels_flat)\n",
    "            \n",
    "            if len(true_index)==0:\n",
    "                acc+=1\n",
    "                print('----passed one error----')\n",
    "                continue\n",
    "\n",
    "            for items in true_index:\n",
    "                flag = 1\n",
    "                for i in items:\n",
    "                    if predicted_labels_flat[i] != true_labels_flat[i]:\n",
    "                        flag = 0\n",
    "                        break\n",
    "                if flag == 1:\n",
    "                    accuracy += 1\n",
    "            accuracy /= len(true_index)\n",
    "\n",
    "            # print(\"----predicted----\")\n",
    "            # print(predicted_labels_flat)\n",
    "            # print(\"----true----\")\n",
    "            # print(true_labels_flat)\n",
    "            # print(\"----acc----\")\n",
    "            # print(accuracy)\n",
    "            acc += accuracy\n",
    "            f1score = f1_score(true_labels_flat, predicted_labels_flat, average='macro')\n",
    "            f1 += f1score\n",
    "\n",
    "    valid_loss /= len(loader)\n",
    "    acc /= len(loader)\n",
    "    f1 /= len(loader)\n",
    "\n",
    "    # 将指标写入TensorBoard\n",
    "    writer0.add_scalar('valid_loss', valid_loss, epoch)\n",
    "    writer0.add_scalar('acc', acc, epoch)\n",
    "    writer0.add_scalar('f1', f1, epoch)\n",
    "\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def main():\n",
    "    labels_path = \"./data/NER/new/labels.json\"\n",
    "    model_name = './model/roberta/'\n",
    "    '''\n",
    "    train_json_path = \"./data/NER/new/train.json\"\n",
    "    val_json_path = \"./data/NER/new/dev.json\"\n",
    "    '''\n",
    "    data_json_path = './data/NER/new/NER_BIO.json'\n",
    "    max_length = 300\n",
    "    epochs = 100\n",
    "    batch_size = 1\n",
    "    lr = 2e-5\n",
    "    gradient_accumulation_steps = 16\n",
    "    model_output_dir = \"./output/NER\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载label\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as r:\n",
    "        labels_map = json.loads(r.read())\n",
    "\n",
    "    # 加载分词器和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_map))\n",
    "    model.to(device)\n",
    "\n",
    "    # 加载数据\n",
    "    '''\n",
    "    print(\"Start Load Train Data...\")\n",
    "    train_dataset = NERDataset(tokenizer, train_json_path, labels_map, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(\"Start Load Validation Data...\")\n",
    "    val_dataset = NERDataset(tokenizer, val_json_path, labels_map, max_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)'''\n",
    "\n",
    "    data = NERDataset(tokenizer, data_json_path, labels_map, max_length)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(data,batch_size = batch_size, shuffle=True)\n",
    "\n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    print(\"Start Training...\")\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch, model, device, test_loader, optimizer, gradient_accumulation_steps)\n",
    "        print(\"Start Validation...\")\n",
    "        acc, f1 = validate(model, device, test_loader,epoch)\n",
    "        print(f\"Validation : acc: {acc} , f1: {f1}\")\n",
    "\n",
    "        if best_acc < acc: # 保存准确率最高的模型\n",
    "            print(\"Save Model To \", model_output_dir)\n",
    "            model.save_pretrained(model_output_dir)\n",
    "            tokenizer.save_pretrained(model_output_dir)\n",
    "            best_acc = acc\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    writer0.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入： 李白城主在天上飞\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'思想': ['李白']}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入： 李白说‘飞流直下三千尺’\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'人名': ['李白'], '思想': ['飞流直下三千尺']}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2484507/4010148022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2484507/4010148022.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"请输入：\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStdinNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1283\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1323\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "# 解析实体\n",
    "def post_processing(outputs, text, labels_map):\n",
    "    _, predicted_labels = torch.max(outputs.logits, dim=2)\n",
    "\n",
    "    predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "\n",
    "    predicted_tags = [labels_map[label_id] for label_id in predicted_labels[0]]\n",
    "\n",
    "    result = {}\n",
    "    entity = \"\"\n",
    "    type = \"\"\n",
    "    for index, word_token in enumerate(text):\n",
    "        tag = predicted_tags[index]\n",
    "        if tag.startswith(\"B-\"):\n",
    "            type = tag.split(\"-\")[1]\n",
    "            if entity:\n",
    "                if type not in result:\n",
    "                    result[type] = []\n",
    "                result[type].append(entity)\n",
    "            entity = word_token\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            type = tag.split(\"-\")[1]\n",
    "            if entity:\n",
    "                entity += word_token\n",
    "        else:\n",
    "            if entity:\n",
    "                if type not in result:\n",
    "                    result[type] = []\n",
    "                result[type].append(entity)\n",
    "            entity = \"\"\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    labels_path = \"./data/NER/new/labels.json\"\n",
    "    model_name = './output/NER/'\n",
    "    max_length = 300\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载label\n",
    "    labels_map = {}\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as r:\n",
    "        labels = json.loads(r.read())\n",
    "        for label in labels:\n",
    "            label_id = labels[label]\n",
    "            labels_map[label_id] = label\n",
    "\n",
    "    # 加载分词器和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_map))\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"请输入：\")\n",
    "        if not text or text == '':\n",
    "            continue\n",
    "        if text == 'q':\n",
    "            break\n",
    "\n",
    "        encoded_input = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        input_ids = torch.tensor([encoded_input['input_ids']]).to(device)\n",
    "        attention_mask = torch.tensor([encoded_input['attention_mask']]).to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        result = post_processing(outputs, text, labels_map)\n",
    "        #print(text)\n",
    "        print(result)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     # 加载数据\n",
    "#     print(\"Start Load Test Data...\")\n",
    "#     with open(\"./data/NER/NER_data.json\", \"r\", encoding=\"utf-8\") as r:\n",
    "#         test_data = r.read().split('\\n')\n",
    "        \n",
    "#     with open(\"./NER_output.txt\",'w') as w:\n",
    "#         for i in test_data:\n",
    "\n",
    "#             if len(i) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             data_dict = json.loads(i)\n",
    "\n",
    "#             encoded_input = tokenizer(data_dict['text'], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "#             input_ids = torch.tensor([encoded_input['input_ids']]).to(device)\n",
    "#             attention_mask = torch.tensor([encoded_input['attention_mask']]).to(device)\n",
    "\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#             result = post_processing(outputs, data_dict['text'], labels_map)\n",
    "\n",
    "# #             print(data_dict['text'])\n",
    "# #             print(f'正确答案：',end=' ')\n",
    "# #             for key in data_dict['label'].keys():\n",
    "# #                 # print(f'{key}',end = ' ')\n",
    "# #                 # print(f\"{list(data_dict['label'][key].keys())[0]}\", end = ' ')\n",
    "# #                 print({key : list(data_dict['label'][key].keys())},end = ' ')\n",
    "# #             print('')\n",
    "\n",
    "# #             print(f'输出：{result}')\n",
    "# #             print('-'*10)\n",
    "#             w.write(data_dict['text'])\n",
    "#             w.write('\\n')\n",
    "#             w.write(f'正确答案：')\n",
    "#             for key in data_dict['label'].keys():\n",
    "#                 # print(f'{key}',end = ' ')\n",
    "#                 # print(f\"{list(data_dict['label'][key].keys())[0]}\", end = ' ')\n",
    "#                 ans = {key : list(data_dict['label'][key].keys())}\n",
    "#                 w.write(str(ans))\n",
    "#                 w.write(' ')\n",
    "#             w.write('\\n')\n",
    "\n",
    "#             w.write(f'输出：{result}')\n",
    "#             w.write('\\n')\n",
    "#             w.write('-'*10)\n",
    "#             w.write('\\n')\n",
    "#     print('----DONE----')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Start Validation...\n",
      "Validation Data:  10%|▉         | 24/250 [00:00<00:03, 59.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:  14%|█▍        | 36/250 [00:00<00:03, 58.22it/s]----passed one error----\n",
      "Validation Data:  17%|█▋        | 43/250 [00:00<00:03, 59.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:  22%|██▏       | 55/250 [00:00<00:03, 59.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:  30%|██▉       | 74/250 [00:01<00:02, 59.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:  40%|███▉      | 99/250 [00:01<00:02, 59.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:  65%|██████▍   | 162/250 [00:02<00:01, 59.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:  81%|████████  | 203/250 [00:03<00:00, 59.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data: 100%|██████████| 250/250 [00:04<00:00, 59.51it/s]\n",
      "Validation : acc: 0.9643333333333333 , f1: 0.9429869934741784 , recall: 0.9762574384365971 , precision: 0.9265939293813006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "#from ner_datasets import NERDataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, labels_map, max_length=300):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels_map = labels_map\n",
    "\n",
    "        self.text_data = []\n",
    "        self.label_data = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as r:\n",
    "            for line in r:\n",
    "                line = json.loads(line)\n",
    "                text = line['text']\n",
    "                label = line['label']\n",
    "                self.text_data.append(text)\n",
    "                self.label_data.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        labels = self.label_data[idx]\n",
    "\n",
    "        # 使用分词器对句子进行处理\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        # 将标签转换为数字编码\n",
    "        label_ids = [self.labels_map[l] for l in labels]\n",
    "\n",
    "        if len(label_ids) > self.max_length:\n",
    "            label_ids = label_ids[0:self.max_length]\n",
    "\n",
    "        if len(label_ids) < self.max_length:\n",
    "            # 标签填充到最大长度\n",
    "            label_ids.extend([0] * (self.max_length - len(label_ids)))\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.LongTensor(label_ids)\n",
    "        }\n",
    "\n",
    "def post_processing(outputs, text, labels_map):\n",
    "    _, predicted_labels = torch.max(outputs.logits, dim=2)\n",
    "\n",
    "    predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "\n",
    "    predicted_tags = [labels_map[label_id] for label_id in predicted_labels[0]]\n",
    "\n",
    "    result = {}\n",
    "    entity = \"\"\n",
    "    type = \"\"\n",
    "    for index, word_token in enumerate(text):\n",
    "        tag = predicted_tags[index]\n",
    "        if tag.startswith(\"B-\"):\n",
    "            type = tag.split(\"-\")[1]\n",
    "            if entity:\n",
    "                if type not in result:\n",
    "                    result[type] = []\n",
    "                result[type].append(entity)\n",
    "            entity = word_token\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            type = tag.split(\"-\")[1]\n",
    "            if entity:\n",
    "                entity += word_token\n",
    "        else:\n",
    "            if entity:\n",
    "                if type not in result:\n",
    "                    result[type] = []\n",
    "                result[type].append(entity)\n",
    "            entity = \"\"\n",
    "    return result\n",
    "\n",
    "def extract_non_zero_sublists(lst):\n",
    "    non_zero_sublists = []\n",
    "    sublist = []\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] != 0:\n",
    "            sublist.append(i)\n",
    "        else:\n",
    "            if sublist:  # Check if the sublist is not empty\n",
    "                non_zero_sublists.append(sublist)\n",
    "            sublist = []  # Reset the sublist\n",
    "\n",
    "    # Check for a non-zero sublist at the end of the list\n",
    "    if sublist:\n",
    "        non_zero_sublists.append(sublist)\n",
    "\n",
    "    return non_zero_sublists\n",
    "\n",
    "\n",
    "def validate(model, device, loader,epoch):\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    f1 = 0\n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(tqdm(loader, file=sys.stdout, desc=\"Validation Data\")):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs.logits, dim=2)\n",
    "\n",
    "            predicted_labels = predicted_labels.detach().cpu().numpy().tolist()\n",
    "            true_labels = labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "            predicted_labels_flat = [label for sublist in predicted_labels for label in sublist]\n",
    "            true_labels_flat = [label for sublist in true_labels for label in sublist]\n",
    "            \n",
    "\n",
    "            # accuracy = (np.array(predicted_labels_flat) == np.array(true_labels_flat)).mean()\n",
    "            accuracy = 0\n",
    "            true_index = extract_non_zero_sublists(true_labels_flat)\n",
    "            \n",
    "            # if(True):\n",
    "            #     # print(true_labels_flat)\n",
    "            #     # print(predicted_labels_flat)\n",
    "            #     # result = post_processing(outputs, data, labels_map)\n",
    "            #     print(data['input_ids'])\n",
    "            #     sys.exit(0)\n",
    "            \n",
    "            if len(true_index)==0:\n",
    "                acc+=1\n",
    "                print('----passed one error----')\n",
    "                continue\n",
    "\n",
    "            for items in true_index:\n",
    "                flag = 1\n",
    "                for i in items:\n",
    "                    if predicted_labels_flat[i] != true_labels_flat[i]:\n",
    "                        flag = 0\n",
    "                        break\n",
    "                if flag == 1:\n",
    "                    accuracy += 1\n",
    "            \n",
    "            accuracy /= len(true_index)\n",
    "            acc += accuracy\n",
    "            f1score = f1_score(true_labels_flat, predicted_labels_flat, average='macro')\n",
    "            r_score = recall_score(true_labels_flat, predicted_labels_flat, average='macro')\n",
    "            p_score = precision_score(true_labels_flat, predicted_labels_flat, average='macro')\n",
    "            f1 += f1score\n",
    "            recall += r_score\n",
    "            precision += p_score\n",
    "\n",
    "    valid_loss /= len(loader)\n",
    "    acc /= len(loader)\n",
    "    f1 /= len(loader)\n",
    "    recall /= len(loader)\n",
    "    precision /= len(loader)\n",
    "\n",
    "    return acc, f1, recall, precision\n",
    "\n",
    "\n",
    "def main():\n",
    "    labels_path = \"./data/NER/new/labels.json\"\n",
    "    # model_name = './model/roberta/'\n",
    "    model_name = './output/NER'\n",
    "    '''\n",
    "    train_json_path = \"./data/NER/new/train.json\"\n",
    "    val_json_path = \"./data/NER/new/dev.json\"\n",
    "    '''\n",
    "    data_json_path = './data/NER/new/NER_BIO.json'\n",
    "    max_length = 300\n",
    "    epochs = 1\n",
    "    batch_size = 1\n",
    "    lr = 1e-5\n",
    "    gradient_accumulation_steps = 16\n",
    "    model_output_dir = \"./output/NER\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载label\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as r:\n",
    "        labels_map = json.loads(r.read())\n",
    "\n",
    "    # 加载分词器和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_map))\n",
    "    model.to(device)\n",
    "\n",
    "    data = NERDataset(tokenizer, data_json_path, labels_map, max_length)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    print(\"Start Training...\")\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # train(epoch, model, device, train_loader, optimizer, gradient_accumulation_steps)\n",
    "        print(\"Start Validation...\")\n",
    "        acc, f1, recall, precision = validate(model, device, test_loader,epoch)\n",
    "        print(f\"Validation : acc: {acc} , f1: {f1} , recall: {recall} , precision: {precision}\")\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
