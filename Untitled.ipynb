{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c37e6-72db-4d01-86cd-2a30ac9e4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# 将数据转为  BIO 标注形式\n",
    "def dimension_label(path, save_path, labels_path=None):\n",
    "    label_dict = ['O']\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as w:\n",
    "        #写入模式\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as r:\n",
    "            for line in r:\n",
    "                line = json.loads(line)\n",
    "                text = line['text']\n",
    "                label = line['label']\n",
    "                text_label = ['O'] * len(text)\n",
    "                for label_key in label:  # 遍历实体标签\n",
    "                    B_label = \"B-\" + label_key\n",
    "                    I_label = \"I-\" + label_key\n",
    "                    if B_label not in label_dict:\n",
    "                        label_dict.append(B_label)\n",
    "                    if I_label not in label_dict:\n",
    "                        label_dict.append(I_label)\n",
    "                    label_item = label[label_key]\n",
    "                    for entity in label_item:  # 遍历实体\n",
    "                        position = label_item[entity]\n",
    "                        '''\n",
    "                        start = position[0][0]\n",
    "                        end = position[0][1]\n",
    "                        print(f\"实体: {entity}, 起始位置: {start}, 结束位置: {end}, 文本: {text}, 文本标签长度: {len(text_label)}\")\n",
    "                        # 检查 start 和 end 是否在 text_label 的范围内\n",
    "                        if start < 0 or end >= len(text_label):\n",
    "                            print(f\"错误：实体 {entity} 的索引超出范围。起始位置: {start}, 结束位置: {end}, 文本长度: {len(text_label)}\")\n",
    "                            continue\n",
    "                        text_label[start] = B_label\n",
    "                        for i in range(start + 1, end + 1):\n",
    "                            text_label[i] = I_label\n",
    "                        '''\n",
    "                        start = position[0][0]\n",
    "                        end = position[0][1]\n",
    "                        text_label[start] = B_label\n",
    "                        for i in range(start + 1, end + 1):\n",
    "                            text_label[i] = I_label\n",
    "                line = {\n",
    "                    \"text\": text,\n",
    "                    \"label\": text_label\n",
    "                }\n",
    "                line = json.dumps(line, ensure_ascii=False)\n",
    "                w.write(line + \"\\n\")\n",
    "                w.flush()\n",
    "\n",
    "    if labels_path:  # 保存 label ，后续训练和预测时使用\n",
    "        label_map = {}\n",
    "        for i,label in enumerate(label_dict):\n",
    "            label_map[label] = i\n",
    "        with open(labels_path, \"w\", encoding=\"utf-8\") as w:\n",
    "            labels = json.dumps(label_map, ensure_ascii=False)\n",
    "            w.write(labels + \"\\n\")\n",
    "            w.flush()\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = \"./data/NER/dev.json\"\n",
    "    save_path = \"./data/NER/new/dev.json\"\n",
    "    dimension_label(path, save_path)\n",
    "\n",
    "    path = \"./data/NER/train.json\"\n",
    "    save_path = \"./data/NER/new/train.json\"\n",
    "    dimension_label(path, save_path)\n",
    "    \n",
    "    labels_path = \"./data/NER/new/labels.json\"\n",
    "    dimension_label('./data/NER/original_data.json','./data/NER/new/original_data.json',labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090bebc-1991-4282-9e1d-174c35f4edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, labels_map, max_length=300):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels_map = labels_map\n",
    "\n",
    "        self.text_data = []\n",
    "        self.label_data = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as r:\n",
    "            for line in r:\n",
    "                line = json.loads(line)\n",
    "                text = line['text']\n",
    "                label = line['label']\n",
    "                self.text_data.append(text)\n",
    "                self.label_data.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        labels = self.label_data[idx]\n",
    "\n",
    "        # 使用分词器对句子进行处理\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        # 将标签转换为数字编码\n",
    "        label_ids = [self.labels_map[l] for l in labels]\n",
    "\n",
    "        if len(label_ids) > self.max_length:\n",
    "            label_ids = label_ids[0:self.max_length]\n",
    "\n",
    "        if len(label_ids) < self.max_length:\n",
    "            # 标签填充到最大长度\n",
    "            label_ids.extend([0] * (self.max_length - len(label_ids)))\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.LongTensor(label_ids)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6dd6f8-aaf1-4fa1-85d9-ac8bf2c06d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "# 解析实体\n",
    "def post_processing(outputs, text, labels_map):\n",
    "    _, predicted_labels = torch.max(outputs.logits, dim=2)\n",
    "\n",
    "    predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "\n",
    "    predicted_tags = [labels_map[label_id] for label_id in predicted_labels[0]]\n",
    "\n",
    "    result = {}\n",
    "    entity = \"\"\n",
    "    type = \"\"\n",
    "    for index, word_token in enumerate(text):\n",
    "        tag = predicted_tags[index]\n",
    "        if tag.startswith(\"B-\"):\n",
    "            type = tag.split(\"-\")[1]\n",
    "            if entity:\n",
    "                if type not in result:\n",
    "                    result[type] = []\n",
    "                result[type].append(entity)\n",
    "            entity = word_token\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            type = tag.split(\"-\")[1]\n",
    "            if entity:\n",
    "                entity += word_token\n",
    "        else:\n",
    "            if entity:\n",
    "                if type not in result:\n",
    "                    result[type] = []\n",
    "                result[type].append(entity)\n",
    "            entity = \"\"\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    labels_path = \"./data/NER/new/labels.json\"\n",
    "    model_name = './output/NER/'\n",
    "    max_length = 300\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载label\n",
    "    labels_map = {}\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as r:\n",
    "        labels = json.loads(r.read())\n",
    "        for label in labels:\n",
    "            label_id = labels[label]\n",
    "            labels_map[label_id] = label\n",
    "\n",
    "    # 加载分词器和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_map))\n",
    "    model.to(device)\n",
    "\n",
    "    '''\n",
    "    while True:\n",
    "        text = input(\"请输入：\")\n",
    "        if not text or text == '':\n",
    "            continue\n",
    "        if text == 'q':\n",
    "            break\n",
    "\n",
    "        encoded_input = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        input_ids = torch.tensor([encoded_input['input_ids']]).to(device)\n",
    "        attention_mask = torch.tensor([encoded_input['attention_mask']]).to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        result = post_processing(outputs, text, labels_map)\n",
    "        #print(text)\n",
    "        print(result)\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    # 加载数据\n",
    "    print(\"Start Load Test Data...\")\n",
    "    with open(\"./data/NER/test.json\", \"r\", encoding=\"utf-8\") as r:\n",
    "        test_data = r.read().split('\\n')\n",
    "    for i in test_data:\n",
    "\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        \n",
    "        data_dict = json.loads(i)\n",
    "        \n",
    "        encoded_input = tokenizer(data_dict['text'], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        input_ids = torch.tensor([encoded_input['input_ids']]).to(device)\n",
    "        attention_mask = torch.tensor([encoded_input['attention_mask']]).to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        result = post_processing(outputs, data_dict['text'], labels_map)\n",
    "\n",
    "        print(data_dict['text'])\n",
    "        print(f'正确答案：',data_dict['label'],sep = '')\n",
    "\n",
    "        print(f'输出：{result}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6929160f-01cb-42c3-97e7-1f177992c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.25.1\n",
      "  Downloading Levenshtein-0.25.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 65 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.8.0\n",
      "  Downloading rapidfuzz-3.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 63 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.25.1 python-Levenshtein-0.25.1 rapidfuzz-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0afe92a-8111-4e5b-9dfc-1e42ef2e7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似度：67%\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "string1 = \"凿空”西域,通西域。\"\n",
    "string2 = \"凿空”西域\"\n",
    "similarity_ratio = fuzz.ratio(string1, string2)\n",
    "print(f\"相似度：{similarity_ratio}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
